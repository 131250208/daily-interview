1. LSTM 网络结构？

    LSTM 即长短时记忆网络，包括三个门：更新门（输入门）、遗忘门和输出门。公式如下：
    $$
    \hat{c}^{<t>} = \tanh (W_c [a^{<t-1}>, x^{<t>}] + b_c) \\
    \Gamma_u = \sigma(W_u [a^{<t-1}>, x^{<t>}] + b_u) \\
    \Gamma_f = \sigma(W_f [a^{<t-1}>, x^{<t>}] + b_f) \\
    \Gamma_o = \sigma(W_o [a^{<t-1}>, x^{<t>}] + b_o) \\
    c^{<t>} = \Gamma_u * \hat{c}^{<t>} + \Gamma_f*c^{<t-1>} \\
    a^{<t>} = \Gamma_o * c^{<t>}
    $$

2. 假设输入维度为 m，输出为 n，求 GRU 参数？

    输入  W：3nm，隐层 W：3nn，隐层 b：3n，合计共：`3*(nn+nm+n)`。当然，也有的实现会把前一时刻的隐层和当前时刻的输入分开，使用两个 bias，此时需要再增加 3n 个参数。

3. CNN 有什么好处？

    - 稀疏（局部）连接：卷积核尺寸远小于输入特征尺寸，输出层的每个节点都只与部分输入层连接
    - 参数共享：卷积核的滑动窗在不同位置的权值是一样的
    - 等价表示（输入/输出数据的结构化）：输入和输出在结构上保持对应关系

4. 卷积层输出 size？

    给定 n×n 输入，f×f 卷积核，padding p，stride s，输出的尺寸为：
    $$
    \lfloor \frac{n+2p-f}{s} + 1 \rfloor \times \lfloor \frac{n+2p-f}{s} + 1 \rfloor
    $$
    
5. Attention 机制

    Attention 核心是从输入中有选择地聚焦到特定重要信息上的一种机制。有三种不同用法：

    - 在 encoder-decoder attention 层，query 来自上一个 decoder layer，memory keys 和 values 来自 encoder 的 output
    - encoder 包含 self-attention，key value 和 query 来自相同的位置，即前一层的输出。encoder 的每个位置都可以注意到前一层的所有位置
    - decoder 与 encoder 类似，通过将所有不合法连接 mask 以防止信息溢出

6. 孪生网络原理？

    孪生网络是指包含两个或多个相同子网络的架构。相同是指配置、参数和权重都一模一样。主要用于评价输入之间的相似度。损失函数主要采用 Triplet Loss 或 Contrastive Loss。
    
7. FastText 相比 Word2vec 有哪些不同？

    - FastText 增加了 Ngram 特征，可以更好地解决未登录词及在小数据集上训练的问题
    - FastText 是一个工具包，除了可以训练词向量还可以训练有监督的文本分类模型

8. Glove 与 Word2vec 的区别？

    在 Word2vec 中，高频的词共现只是产生了更多的训练数据，并没有携带额外的信息；Glove 加入词的全局共现频率信息。它基于词上下文矩阵的矩阵分解技术，首先构建一个大的单词×上下文共现矩阵，然后学习低维表示，可以视为共现矩阵的重构问题。

    - Word2vec 是局部语料训练，特征提取基于滑动窗口；Glove 的滑动窗口是为了构建共现矩阵，统计全部语料在固定窗口内词的共现频次。
    - Word2vec 损失函数是带权重的交叉熵；Glove 的损失函数是最小平方损失
    - Glove 利用了全局信息，训练时收敛更快
    
9. Doc2vec 原理？

    Doc2vec 是训练文档表征的，在输入层增加了一个 Doc 向量。有两种不同的训练方法：Distributed Memory  是给定上下文和段落向量的情况下预测单词的概率。在一个句子或者段落文档训练过程中，段落 ID 保存不变，共享同一个段落向量。Distributed Bag of Words 则在只给定段落向量的情况下预测段落中一组随机单词的概率。使用时固定词向量，随机初始化 Doc 向量，训练几个步骤后得到最终 Doc 向量。
    
10. LSTM 和 GRU 的区别？

    - GRU 将 LSTM 的更新门、遗忘门和输出门替换为更新门和重置门
    - GRU 将记忆状态和输出状态合并为一个状态
    - GRU 参数更少，更容易收敛，但数据量大时，LSTM 效果更好
    
11. 自注意力中为何要缩放？

    维度较大时，向量内积容易使得 SoftMax 将概率全部分配给最大值对应的 Label，其他 Label 的概率几乎为 0，反向传播时这些梯度会变得很小甚至为 0，导致无法更新参数。因此，一般会对其进行缩放，缩放值一般使用维度 dk 开根号，是因为点积的方差是 dk，缩放后点积的方差为常数 1，这样就可以避免梯度消失问题。

    另外，Hinton 等人的研究发现，在知识蒸馏过程中，学生网络以一种略微不同的方式从教师模型中抽取知识，它使用大模型在现有标记数据上生成软标签，而不是硬的二分类。直觉是软标签捕获了不同类之间的关系，这是大模型所没有的。这里的软标签就是缩放的 SoftMax。

    至于为啥最后一层为啥一般不需要缩放，因为最后输出的一般是分类结果，参数更新不需要继续传播，自然也就不会有梯度消失的问题。
    
12. ELMO？

    使用双向语言模型建模，两层 LSTM 分别学习语法和语义特征。首次使用两阶段训练方法，训练后可以在下游任务微调。

    Feature-Based 微调，预训练模型作为纯粹的表征抽取器，表征依赖微调任务网络结构适配（任务缩放因子 γ）。

    ELMO 的缺点主要包括：不完全的双向预训练（Bi 是分开的，仅在 Loss 合并）；需要进行任务相关的网络设计（每种下游任务都要特定的设计）；仅有词向量无句向量（没有句向量任务）。
    
13. GPT？

    使用 Transformer 的 Decoder 替换 LSTM 作为特征提取器。

    Model-Based 微调，预训练模型作为任务网络的一部分参与任务学习，简化了下游任务架构设计。

    GPT 的缺点包括：单项预训练模型；仅有词向量无句向量（仅学习语言模型）。

14. BERT？

    使用 Transformer Encoder 作为特征提取器，交互式双向语言建模（MLM），Token 级别+句子级别任务（MLM+NSP），两阶段预训练。

    Feature-Based 和 Model-Based，实际一般使用 Model-Based。

    BERT 的缺点是：字粒度难以学到词、短语、实体的完整语义。

15. ERNIE：

    对 BERT 的缺点进行了优化，Mask 从字粒度的 Token 修改为完整的词或实体。ERNIE2.0 引入更多的预训练任务以捕捉更丰富的语义知识。

16. Transformer 中为什么用 Add 而不是 Concat？

    在 Embedding 中，Add 等价于 Concat，三个 Embedding 相加与分别 One-Hot Concat 效果相同。

17. Dropout 作用、原理和实现？

